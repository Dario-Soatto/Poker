{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645f8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "\n",
    "import numpy as np\n",
    "import math as math\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from autils import *\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec7a612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input training is: (88675, 367)\n",
      "The shape of the output training is: (88675, 4)\n",
      "The shape of the input validation is: (846, 367)\n",
      "The shape of the output validation is: (846, 4)\n"
     ]
    }
   ],
   "source": [
    "#Reading all of the csv files. The csv files are stored as pandas dataframes, which are then converted to numpy arrays\n",
    "#During the conversion process, the first row of the data is essentially lost; keep this in mind when trying to match inputs/outputs to the csv\n",
    "df1 = pd.read_csv('input_data_2.csv')\n",
    "inputs = df1.to_numpy()\n",
    "\n",
    "df2 = pd.read_csv('output_data_2.csv')\n",
    "outputs = df2.to_numpy()\n",
    "\n",
    "print ('The shape of the input training is: ' + str(inputs.shape))\n",
    "print ('The shape of the output training is: ' + str(outputs.shape))\n",
    "\n",
    "df3 = pd.read_csv('input_validation_data_2.csv')\n",
    "testIn = df3.to_numpy()\n",
    "df4 = pd.read_csv('output_validation_data_2.csv')\n",
    "testOut = df4.to_numpy()\n",
    "\n",
    "print ('The shape of the input validation is: ' + str(testIn.shape))\n",
    "print ('The shape of the output validation is: ' + str(testOut.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b3efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a neural network. There are 367 inputs and 4 outputs. The layers can be altered as desired.\n",
    "model = Sequential(\n",
    "    [               \n",
    "        tf.keras.Input(shape=(367,)),    #specify input size\n",
    "        ### START CODE HERE ### \n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(150, activation='relu'),\n",
    "        Dense(200, activation='relu'),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dense(400, activation='relu'),\n",
    "        Dense(400, activation='relu'),\n",
    "        Dense(400, activation='relu'),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dense(200, activation='relu'),\n",
    "        Dense(150, activation='relu'),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ### \n",
    "    ], name = \"my_model\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7854cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 25)                9200      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               2600      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 150)               15150     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 200)               30200     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 300)               60300     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 400)               120400    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 400)               160400    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 400)               160400    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 300)               120300    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875104 (3.34 MB)\n",
      "Trainable params: 875104 (3.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Printing the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f27c96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.8611\n",
      "Epoch 2/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.7673\n",
      "Epoch 3/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.7168\n",
      "Epoch 4/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.7004\n",
      "Epoch 5/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.6407\n",
      "Epoch 6/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.6220\n",
      "Epoch 7/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.6066\n",
      "Epoch 8/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5870\n",
      "Epoch 9/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5673\n",
      "Epoch 10/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.5544\n",
      "Epoch 11/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5587\n",
      "Epoch 12/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5327\n",
      "Epoch 13/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5327\n",
      "Epoch 14/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5313\n",
      "Epoch 15/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5133\n",
      "Epoch 16/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5105\n",
      "Epoch 17/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.5012\n",
      "Epoch 18/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5144\n",
      "Epoch 19/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4894\n",
      "Epoch 20/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4941\n",
      "Epoch 21/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4793\n",
      "Epoch 22/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4800\n",
      "Epoch 23/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4724\n",
      "Epoch 24/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4680\n",
      "Epoch 25/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4680\n",
      "Epoch 26/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4653\n",
      "Epoch 27/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4645\n",
      "Epoch 28/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4577\n",
      "Epoch 29/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4720\n",
      "Epoch 30/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4521\n",
      "Epoch 31/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4494\n",
      "Epoch 32/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4462\n",
      "Epoch 33/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4476\n",
      "Epoch 34/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4399\n",
      "Epoch 35/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4396\n",
      "Epoch 36/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4327\n",
      "Epoch 37/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4321\n",
      "Epoch 38/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4291\n",
      "Epoch 39/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4285\n",
      "Epoch 40/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4359\n",
      "Epoch 41/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4214\n",
      "Epoch 42/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4242\n",
      "Epoch 43/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4255\n",
      "Epoch 44/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4148\n",
      "Epoch 45/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4194\n",
      "Epoch 46/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4108\n",
      "Epoch 47/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4123\n",
      "Epoch 48/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4083\n",
      "Epoch 49/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4038\n",
      "Epoch 50/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4029\n",
      "Epoch 51/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4019\n",
      "Epoch 52/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.5504\n",
      "Epoch 53/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3962\n",
      "Epoch 54/250\n",
      "2772/2772 [==============================] - 10s 4ms/step - loss: 0.5161\n",
      "Epoch 55/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4544\n",
      "Epoch 56/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4278\n",
      "Epoch 57/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4134\n",
      "Epoch 58/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3974\n",
      "Epoch 59/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3988\n",
      "Epoch 60/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3913\n",
      "Epoch 61/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.4103\n",
      "Epoch 62/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4081\n",
      "Epoch 63/250\n",
      "2772/2772 [==============================] - 7s 3ms/step - loss: 0.3930\n",
      "Epoch 64/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3882\n",
      "Epoch 65/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3888\n",
      "Epoch 66/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4012\n",
      "Epoch 67/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3868\n",
      "Epoch 68/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3764\n",
      "Epoch 69/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3742\n",
      "Epoch 70/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3736\n",
      "Epoch 71/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3710\n",
      "Epoch 72/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3656\n",
      "Epoch 73/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3684\n",
      "Epoch 74/250\n",
      "2772/2772 [==============================] - 7s 3ms/step - loss: 0.3650\n",
      "Epoch 75/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3604\n",
      "Epoch 76/250\n",
      "2772/2772 [==============================] - 377s 136ms/step - loss: 0.3610\n",
      "Epoch 77/250\n",
      "2772/2772 [==============================] - 63s 23ms/step - loss: 0.3590\n",
      "Epoch 78/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3609\n",
      "Epoch 79/250\n",
      "2772/2772 [==============================] - 106s 38ms/step - loss: 0.3544\n",
      "Epoch 80/250\n",
      "2772/2772 [==============================] - 1003s 362ms/step - loss: 0.3537\n",
      "Epoch 81/250\n",
      "2772/2772 [==============================] - 64s 23ms/step - loss: 0.3499\n",
      "Epoch 82/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3530\n",
      "Epoch 83/250\n",
      "2772/2772 [==============================] - 965s 348ms/step - loss: 0.3507\n",
      "Epoch 84/250\n",
      "2772/2772 [==============================] - 1008s 364ms/step - loss: 0.3493\n",
      "Epoch 85/250\n",
      "2772/2772 [==============================] - 44s 16ms/step - loss: 0.3449\n",
      "Epoch 86/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3496\n",
      "Epoch 87/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3456\n",
      "Epoch 88/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3431\n",
      "Epoch 89/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3403\n",
      "Epoch 90/250\n",
      "2772/2772 [==============================] - 10s 3ms/step - loss: 0.3404\n",
      "Epoch 91/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3333\n",
      "Epoch 92/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3382\n",
      "Epoch 93/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3348\n",
      "Epoch 94/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3321\n",
      "Epoch 95/250\n",
      "2772/2772 [==============================] - 10s 3ms/step - loss: 0.3300\n",
      "Epoch 96/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3477\n",
      "Epoch 97/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3277\n",
      "Epoch 98/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3232\n",
      "Epoch 99/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3338\n",
      "Epoch 100/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3285\n",
      "Epoch 101/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3227\n",
      "Epoch 102/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3167\n",
      "Epoch 103/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3242\n",
      "Epoch 104/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3115\n",
      "Epoch 105/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3156\n",
      "Epoch 106/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3254\n",
      "Epoch 107/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3108\n",
      "Epoch 108/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3113\n",
      "Epoch 109/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3297\n",
      "Epoch 110/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3208\n",
      "Epoch 111/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3060\n",
      "Epoch 112/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3233\n",
      "Epoch 113/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3026\n",
      "Epoch 114/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3199\n",
      "Epoch 115/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3051\n",
      "Epoch 116/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3026\n",
      "Epoch 117/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3074\n",
      "Epoch 118/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3058\n",
      "Epoch 119/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2947\n",
      "Epoch 120/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2990\n",
      "Epoch 121/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3055\n",
      "Epoch 122/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3435\n",
      "Epoch 123/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2914\n",
      "Epoch 124/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2920\n",
      "Epoch 125/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2954\n",
      "Epoch 126/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2927\n",
      "Epoch 127/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2946\n",
      "Epoch 128/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2915\n",
      "Epoch 129/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2887\n",
      "Epoch 130/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2928\n",
      "Epoch 131/250\n",
      "2772/2772 [==============================] - 10s 3ms/step - loss: 0.2816\n",
      "Epoch 132/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2947\n",
      "Epoch 133/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2905\n",
      "Epoch 134/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2769\n",
      "Epoch 135/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2879\n",
      "Epoch 136/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2839\n",
      "Epoch 137/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.3099\n",
      "Epoch 138/250\n",
      "2772/2772 [==============================] - 13s 5ms/step - loss: 0.2798\n",
      "Epoch 139/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2747\n",
      "Epoch 140/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2753\n",
      "Epoch 141/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2758\n",
      "Epoch 142/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2971\n",
      "Epoch 143/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2727\n",
      "Epoch 144/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2705\n",
      "Epoch 145/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2792\n",
      "Epoch 146/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2888\n",
      "Epoch 147/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2738\n",
      "Epoch 148/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2699\n",
      "Epoch 149/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2912\n",
      "Epoch 150/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2632\n",
      "Epoch 151/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2632\n",
      "Epoch 152/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2660\n",
      "Epoch 153/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2749\n",
      "Epoch 154/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2648\n",
      "Epoch 155/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2996\n",
      "Epoch 156/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2635\n",
      "Epoch 157/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2549\n",
      "Epoch 158/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2595\n",
      "Epoch 159/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2648\n",
      "Epoch 160/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2565\n",
      "Epoch 161/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2759\n",
      "Epoch 162/250\n",
      "2772/2772 [==============================] - 32s 11ms/step - loss: 0.2588\n",
      "Epoch 163/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2585\n",
      "Epoch 164/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2526\n",
      "Epoch 165/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2617\n",
      "Epoch 166/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.3729\n",
      "Epoch 167/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2527\n",
      "Epoch 168/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2502\n",
      "Epoch 169/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2508\n",
      "Epoch 170/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2490\n",
      "Epoch 171/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2457\n",
      "Epoch 172/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2882\n",
      "Epoch 173/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2526\n",
      "Epoch 174/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2986\n",
      "Epoch 175/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2474\n",
      "Epoch 176/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2488\n",
      "Epoch 177/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2459\n",
      "Epoch 178/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2456\n",
      "Epoch 179/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2450\n",
      "Epoch 180/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 1.2178\n",
      "Epoch 181/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2726\n",
      "Epoch 182/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2524\n",
      "Epoch 183/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2478\n",
      "Epoch 184/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2410\n",
      "Epoch 185/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2511\n",
      "Epoch 186/250\n",
      "2772/2772 [==============================] - 147s 53ms/step - loss: 0.2459\n",
      "Epoch 187/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2638\n",
      "Epoch 188/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2494\n",
      "Epoch 189/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2493\n",
      "Epoch 190/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2609\n",
      "Epoch 191/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2362\n",
      "Epoch 192/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2383\n",
      "Epoch 193/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2453\n",
      "Epoch 194/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2461\n",
      "Epoch 195/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2509\n",
      "Epoch 196/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2548\n",
      "Epoch 197/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2673\n",
      "Epoch 198/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2985\n",
      "Epoch 199/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2597\n",
      "Epoch 200/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2428\n",
      "Epoch 201/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2572\n",
      "Epoch 202/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2614\n",
      "Epoch 203/250\n",
      "2772/2772 [==============================] - 212s 76ms/step - loss: 0.2297\n",
      "Epoch 204/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2851\n",
      "Epoch 205/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2338\n",
      "Epoch 206/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2287\n",
      "Epoch 207/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2352\n",
      "Epoch 208/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2481\n",
      "Epoch 209/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2345\n",
      "Epoch 210/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2315\n",
      "Epoch 211/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2295\n",
      "Epoch 212/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2261\n",
      "Epoch 213/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2531\n",
      "Epoch 214/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2291\n",
      "Epoch 215/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2356\n",
      "Epoch 216/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2625\n",
      "Epoch 217/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2243\n",
      "Epoch 218/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2229\n",
      "Epoch 219/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2349\n",
      "Epoch 220/250\n",
      "2772/2772 [==============================] - 20s 7ms/step - loss: 0.2781\n",
      "Epoch 221/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2272\n",
      "Epoch 222/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2233\n",
      "Epoch 223/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.4252\n",
      "Epoch 224/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2535\n",
      "Epoch 225/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2296\n",
      "Epoch 226/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2451\n",
      "Epoch 227/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2368\n",
      "Epoch 228/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2299\n",
      "Epoch 229/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2393\n",
      "Epoch 230/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2309\n",
      "Epoch 231/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2214\n",
      "Epoch 232/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2475\n",
      "Epoch 233/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2170\n",
      "Epoch 234/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2423\n",
      "Epoch 235/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2235\n",
      "Epoch 236/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2123\n",
      "Epoch 237/250\n",
      "2772/2772 [==============================] - 915s 330ms/step - loss: 0.2295\n",
      "Epoch 238/250\n",
      "2772/2772 [==============================] - 1069s 386ms/step - loss: 0.2555\n",
      "Epoch 239/250\n",
      "2772/2772 [==============================] - 1275s 460ms/step - loss: 0.2114\n",
      "Epoch 240/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2148\n",
      "Epoch 241/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2424\n",
      "Epoch 242/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2296\n",
      "Epoch 243/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2413\n",
      "Epoch 244/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2297\n",
      "Epoch 245/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 3.4951\n",
      "Epoch 246/250\n",
      "2772/2772 [==============================] - 9s 3ms/step - loss: 0.2651\n",
      "Epoch 247/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2478\n",
      "Epoch 248/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2181\n",
      "Epoch 249/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2242\n",
      "Epoch 250/250\n",
      "2772/2772 [==============================] - 8s 3ms/step - loss: 0.2292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x157e806a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model with 250 epochs. The legacy optimizer is used for Mac computers with M1/M2 chips.\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
    ")\n",
    "model.fit(\n",
    "    inputs,outputs,\n",
    "    epochs=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca61e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "163a5de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47536689, 0.1748777 , 0.1748777 , 0.1748777 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([2,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00363195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to encode an input for the neural network\n",
    "    #p: seat\n",
    "    #p1: players left\n",
    "    #bet: highest bet\n",
    "    #h1: first hole card number\n",
    "    #hs1: first hole card suit\n",
    "    #h2: second hole card number\n",
    "    #hs2: second hole card suit\n",
    "    #c1: first community card number\n",
    "    #cs1: first community card suit\n",
    "    #c2: second community card number\n",
    "    #cs2: second community card suit\n",
    "    #c3: third community card number\n",
    "    #cs3: third community card suit\n",
    "    #c4: fourth community card number\n",
    "    #cs4: fourth community card suit\n",
    "    #c5: fifth community card number\n",
    "    #cs5: fifth community card suit\n",
    "    #Card numbers are integers. Ace is low (1)\n",
    "    #Card suits are as follows: spades is 1, clubs is 2, diamonds is 3, hearts is 4\n",
    "\n",
    "def encode_input(p, pl, bet, h1, hs1, h2, hs2, c1, cs1, c2, cs2, c3, cs3, c4, cs4, c5, cs5):\n",
    "    inp = [0.0] * 367\n",
    "    inp[0] = p\n",
    "    inp[1] = pl\n",
    "    inp[2] = math.log(bet, 5)\n",
    "    inp[2 + h1 + 13 * (hs1-1)] = 1\n",
    "    inp[2 + 52 + h2 + 13 * (hs2-1)] = 1\n",
    "    if c1 != 0:\n",
    "        inp[2 + 52 * 2 + c1 + 13 * (cs1-1)] = 1\n",
    "    if c2 != 0:\n",
    "        inp[2 + 52 * 3 + c2 + 13 * (cs2-1)] = 1\n",
    "    if c3 != 0:\n",
    "        inp[2 + 52 * 4 + c3 + 13 * (cs3-1)] = 1\n",
    "    if c4 != 0:\n",
    "        inp[2 + 52 * 5 + c4 + 13 * (cs4-1)] = 1\n",
    "    if c5 != 0:\n",
    "        inp[2 + 52 * 6 + c5 + 13 * (cs5-1)] = 1\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5217352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 960us/step\n",
      "Predicted: [0.29482624 0.18862145 0.18874186 0.32781044], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.17517973 0.47421855 0.17522417 0.17537752], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.27056938 0.18775763 0.3536284  0.1880446 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18073098 0.43391597 0.1807308  0.20462224], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18180804 0.4249431  0.18180794 0.21144098], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17854625 0.4588997  0.1849645  0.17758952], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.20416217 0.18087733 0.4331558  0.1818047 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.4721797  0.17537822 0.17537864 0.17706348], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.4721797  0.17537822 0.17537864 0.17706348], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.46995863 0.17580023 0.17672978 0.17751135], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.467652   0.1760787  0.17609638 0.18017295], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.3720666  0.18669416 0.254269   0.18697026], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.19045264 0.3920468  0.23180453 0.18569604], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.4524592  0.18168513 0.18403429 0.18182139], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.29781508 0.18870153 0.32446316 0.18902025], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18559326 0.1774853  0.45852903 0.17839235], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.1749016  0.17498122 0.1749016  0.47521555], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.20004813 0.18024401 0.43846682 0.18124104], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.47536683 0.1748777  0.1748777  0.17487773], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.1960536  0.3756657  0.24084929 0.18743144], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.28726414 0.1884305  0.18855563 0.33574972], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.28107533 0.19017735 0.32330123 0.20544608], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18199235 0.18129326 0.2067484  0.42996597], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17547722 0.4730596  0.17602375 0.17543943], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.31044865 0.19211046 0.25912204 0.23831883], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17654784 0.18240327 0.17654784 0.46450105], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17648235 0.17691539 0.46596843 0.18063383], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18751292 0.41015437 0.2184678  0.18386489], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.18805341 0.43352666 0.1970965  0.18132344], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17494269 0.17515935 0.17494269 0.47495532], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17487784 0.17487827 0.17487784 0.47536606], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.27115944 0.2897091  0.24492626 0.19420521], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.19883399 0.3696891  0.24344373 0.18803316], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.3127659  0.18878357 0.18889573 0.30955485], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17574453 0.46984875 0.17573783 0.17866892], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17500311 0.175422   0.17500311 0.47457182], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18359023 0.40857548 0.18359019 0.22424409], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.21297896 0.33247462 0.26334786 0.19119856], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.17489333 0.17494538 0.17489333 0.47526798], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17487776 0.17487793 0.17487776 0.4753666 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.461709   0.17710769 0.18250385 0.17867948], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.175227   0.17640182 0.175227   0.47314417], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.21695508 0.36381984 0.22998933 0.18923572], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.47410294 0.1750855  0.17508471 0.17572689], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.18017907 0.18395217 0.44684026 0.18902847], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18968832 0.1915139  0.32537088 0.29342693], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18763113 0.35568163 0.18763113 0.26905608], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.1765327  0.1765386  0.18092227 0.46600646], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.1766354  0.17831627 0.1773985  0.4676498 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.1759874  0.17763393 0.17629907 0.4700796 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18550356 0.18551858 0.390209   0.23876889], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.47042704 0.17684434 0.17569822 0.1770304 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.24565384 0.23928231 0.31631023 0.19875365], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.37899616 0.1862998  0.24683265 0.18787141], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17522557 0.47407782 0.17533836 0.17535825], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.1751416  0.47428453 0.1751352  0.17543869], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17487799 0.17487893 0.17487799 0.4753651 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17487781 0.17487817 0.17487781 0.4753662 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.31024522 0.1898352  0.30081192 0.19910766], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17759685 0.45734778 0.17759548 0.18745987], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.1954837  0.37616846 0.24100336 0.18734452], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18177128 0.21119924 0.18177128 0.4252582 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.22744003 0.35652104 0.22608599 0.1899529 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.30767393 0.18911213 0.3112676  0.1919463 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.25921413 0.18707581 0.36625025 0.18745978], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.46888766 0.17588496 0.17588498 0.17934246], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17585814 0.47214037 0.1754244  0.17657715], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18518451 0.23806967 0.18518451 0.39156127], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17805019 0.1897489  0.17805019 0.45415077], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17553337 0.47281563 0.17618084 0.1754702 ], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.2485153  0.29782853 0.25952438 0.19413182], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17740649 0.45867002 0.17740497 0.18651852], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18874998 0.18744187 0.26081863 0.3629895 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17543373 0.4718988  0.17542167 0.1772458 ], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18215618 0.18190326 0.4253091  0.21063145], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17575823 0.4717922  0.1768413  0.17560829], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.1757522  0.17874148 0.1757522  0.46975413], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.19050406 0.39172214 0.23204747 0.18572633], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.23405273 0.18479753 0.1850047  0.39614502], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.28351548 0.20928441 0.3037541  0.20344603], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.338321   0.22921398 0.23788366 0.19458134], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.19374149 0.2244102  0.19033045 0.39151788], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17696905 0.18440299 0.17696905 0.46165887], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17492582 0.17508617 0.17492582 0.4750622 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.33262458 0.1885112  0.2902438  0.18862042], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.20517428 0.23757026 0.3586189  0.19863662], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17490743 0.17500643 0.17490743 0.47517875], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18758738 0.40969032 0.21880719 0.18391511], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17966817 0.44225463 0.17966786 0.19840932], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18675373 0.25564566 0.18675373 0.3708469 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18733157 0.2640006  0.18733157 0.36133626], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.2049992 0.1808711 0.1812303 0.4328994], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.17657192 0.17673723 0.46539533 0.18129551], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.38379174 0.18582705 0.1858977  0.2444835 ], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.4452442  0.17927247 0.17927444 0.19620888], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18510073 0.18483223 0.2338782  0.3961888 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17572892 0.46995148 0.17572205 0.17859758], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.31905216 0.18976411 0.2921001  0.19908366], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.4515601  0.18875347 0.1795947  0.18009172], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.21520957 0.262097   0.32548928 0.1972041 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.21520957 0.262097   0.32548928 0.1972041 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.19793043 0.17967004 0.18006437 0.44233516], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.27247408 0.19991627 0.31966138 0.20794827], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18077768 0.20490743 0.18077768 0.43353721], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17494543 0.17517127 0.17494543 0.47493786], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.47044003 0.17573102 0.17571367 0.17811525], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.2757614  0.18799353 0.18812568 0.34811938], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.3480839  0.18799016 0.18808354 0.27584234], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.4029453  0.18415023 0.18419974 0.22870472], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.23802157 0.18530464 0.39069483 0.18597898], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18657984 0.18611275 0.24598996 0.38131744], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17492214 0.17507023 0.17492214 0.47508547], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.29985088 0.19460438 0.29888943 0.20665535], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.2738487  0.19888157 0.3183761  0.20889366], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.26817524 0.21656501 0.31333044 0.2019293 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17622468 0.46963644 0.17820632 0.17593251], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18682559 0.18643497 0.24991843 0.37682098], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17553394 0.47123525 0.17552425 0.17770657], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.1855035  0.19850025 0.1888361  0.42716014], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.4716198  0.17612794 0.17548521 0.17676708], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.21956813 0.18907057 0.35836264 0.2329987 ], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.45842442 0.17744045 0.17744072 0.18669441], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.2663479  0.30338523 0.23677127 0.19349563], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.17610477 0.17574029 0.47162208 0.17653285], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.1824925  0.43896434 0.19815011 0.18039308], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.24145114 0.29937196 0.26517752 0.19399941], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.4348777  0.18155909 0.18654005 0.1970232 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.47112828 0.1755426  0.17760065 0.17572847], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.35887685 0.23933564 0.21134317 0.19044432], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.21877152 0.3607679  0.23041731 0.19004329], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.24552007 0.3086137  0.25238833 0.19347787], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17812523 0.17592096 0.46927592 0.1766779 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.23247714 0.3306716  0.2448214  0.1920299 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.2341028  0.28647965 0.27977777 0.1996398 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.24587165 0.29840574 0.24536566 0.21035698], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.20875783 0.3639505  0.23804565 0.18924603], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17489442 0.1749501  0.17489442 0.47526106], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18746735 0.358851   0.18746732 0.26621434], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17521663 0.4741079  0.17531548 0.17535996], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17862241 0.4585345  0.18519922 0.1776439 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17495807 0.17522612 0.17495807 0.4748578 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.25886667 0.18705511 0.36661783 0.1874604 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.19605839 0.18332922 0.20465432 0.41595808], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.24382308 0.19260304 0.29141322 0.27216068], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17594017 0.17570268 0.4717235  0.17663369], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.189554   0.18854186 0.28418702 0.3377171 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.17527431 0.17531857 0.17587796 0.47352913], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.19849952 0.37347037 0.24023005 0.18780008], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.24662195 0.18604194 0.18621084 0.38112533], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.24286982 0.29226    0.26615494 0.19871525], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.19142638 0.21110089 0.39627767 0.20119503], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18774031 0.1879534  0.26259473 0.3617116 ], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17771107 0.46478906 0.17652841 0.18097149], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.20459561 0.21068668 0.38890776 0.19580998], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.17786714 0.45545402 0.17786585 0.18881299], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18286309 0.18273793 0.21694669 0.41745228], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.18510678 0.17915188 0.1880569  0.44768444], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.30459124 0.19600828 0.27658433 0.22281614], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17596853 0.17781712 0.17627248 0.46994185], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.45529798 0.17838372 0.18158466 0.18473366], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.24055277 0.30417475 0.26160723 0.19366524], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.4401464  0.19458067 0.18394747 0.18132544], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.461882   0.17702906 0.17712168 0.18396723], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18148564 0.17662296 0.46439102 0.17750035], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.20355898 0.18074872 0.43406874 0.18162353], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18021144 0.43805066 0.18021114 0.20152673], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.17488402 0.17490503 0.17488402 0.47532693], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17756082 0.17748603 0.18664181 0.4583113 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17487888 0.17488275 0.17487888 0.47535953], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.18515038 0.23774083 0.18515038 0.39195842], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.18085273 0.44750124 0.19241324 0.17923278], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.17603235 0.46795154 0.17602779 0.1799883 ], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.18557034 0.18512838 0.39306632 0.236235  ], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.1858519  0.18553978 0.38847616 0.24013217], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.47313103 0.17523086 0.17622517 0.17541291], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.33105427 0.1910027  0.22064812 0.25729492], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.18167646 0.17713664 0.46296203 0.17822488], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.3002355  0.18871278 0.1888308  0.32222095], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.31161857 0.18878506 0.18889779 0.31069857], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.3368538  0.19089535 0.23056757 0.24168321], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.20204805 0.18054383 0.4359236  0.18148454], actual: [1. 0. 0. 0.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.33512077 0.18844491 0.18854553 0.2878888 ], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.17546484 0.4731126  0.17598958 0.17543294], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.18575604 0.18567221 0.38769364 0.2408781 ], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.21297611 0.18222164 0.42172772 0.18307452], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.46950233 0.17579083 0.17579083 0.17891599], actual: [0. 0. 0. 1.]\n",
      "Predicted: [0.1748777 0.1748777 0.1748777 0.4753669], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17498092 0.17532553 0.17498092 0.4747126 ], actual: [0. 0. 1. 0.]\n",
      "Predicted: [0.18685983 0.2570722  0.18685983 0.36920813], actual: [0. 1. 0. 0.]\n",
      "Predicted: [0.17493048 0.17510642 0.17493048 0.4750326 ], actual: [1. 0. 0. 0.]\n",
      "Correct: 645  Incorrect: 201 accuracy: 0.7624113475177305\n"
     ]
    }
   ],
   "source": [
    "#Evaluating accuracy with a cross-validation set\n",
    "test_predictions = model.predict(testIn)\n",
    "\n",
    "correct = 0\n",
    "for i, pred in enumerate(test_predictions):\n",
    "    if np.argmax(pred) == np.argmax(testOut[i]):\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(f\"Predicted: {softmax(pred)}, actual: {testOut[i]}\")\n",
    "\n",
    "print(f\"Correct: {correct}  Incorrect: {len(test_predictions) - correct} accuracy: {correct / len(test_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae4badd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with the following inputs: Seat 3, 2 players left, 100 in pot, Kc Ah hole, 6c Qd 2s Ad community after the turn\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " predicting with the encoded input: [[1.4057128e-30 2.2245264e-04 0.0000000e+00 9.9977762e-01]]\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      " predicting from the validation data: [[1.4057128e-30 2.2245264e-04 0.0000000e+00 9.9977762e-01]]\n",
      " AI's action from the validation data: [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Testing the model and the input encoding function using an input from the validation data\n",
    "print(\"Testing with the following inputs: Seat 3, 2 players left, 100 in pot, Kc Ah hole, 6c Qd 2s Ad community after the turn\")\n",
    "x = np.array(encode_input(3, 2, 100, 13, 2, 1, 4, 6, 2, 12, 3, 2, 1, 1, 3, 0, 0))\n",
    "prediction = model.predict(x.reshape(1,367))\n",
    "print(f\" predicting with the encoded input: {prediction}\")\n",
    "prediction = model.predict(testIn[143].reshape(1,367))\n",
    "print(f\" predicting from the validation data: {prediction}\")\n",
    "print(f\" AI's action from the validation data: {testOut[143]}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ac6f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      " predicting with the encoded input: [[9.9381548e-04 2.1650227e-04 6.9906205e-02 9.2888355e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Place any set of input here to receive the model output!\n",
    "x = np.array(encode_input(3, 5, 200, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "prediction = model.predict(x.reshape(1,367))\n",
    "print(f\" predicting with the encoded input: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
